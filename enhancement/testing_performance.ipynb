{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from customrec_engine import engines_list, engines, RecommendationAbstract\n",
    "import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List, Tuple\n",
    "\n",
    "product_datas = [{\n",
    "    \"data_context\": \"books\",\n",
    "    \"product_filepath\": \"data/products_books_v1_10_10.csv\",\n",
    "    \"transactions_filepath\": \"data/transactions_books_v1_10_10.csv\",\n",
    "    \"features\": [\"product_title\", \"product_image\", \"product_soup\", \"product_images\"],\n",
    "    \"version\": \"1.0\",\n",
    "    \"unique_name\": \"_books_v1_10_10\",\n",
    "}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_context': 'books',\n",
      " 'features': ['product_title',\n",
      "              'product_image',\n",
      "              'product_soup',\n",
      "              'product_images'],\n",
      " 'product_filepath': 'data/products_books_v1_10_10.csv',\n",
      " 'transactions_filepath': 'data/transactions_books_v1_10_10.csv',\n",
      " 'unique_name': '_books_v1_10_10',\n",
      " 'version': '1.0'}\n",
      "looking at ../data/products_books_v1_10_10.csv\n",
      "WordVec\n",
      "id_to_products length 15443\n",
      "recommendation_ids ['0451526341', '051513628X', '0151008116', '0452280621', '0156006529', '0425145638', '0385484518', '0316899984', '0446356832', '0156005891']\n",
      "recommendation_ids ['0553582127', '0805063897', '1558745157', '0380813815', '0767900383', '0425169863', '1878424319', '0440998050', '0312966970', '0671888587']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [100, 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m         hits\u001b[38;5;241m.\u001b[39mappend(hit)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Measure the performance of the recommendation engine\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpected_hits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m precision \u001b[38;5;241m=\u001b[39m precision_score(expected_hits, hits)\n\u001b[0;32m     69\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(expected_hits, hits)\n",
      "File \u001b[1;32mc:\\github\\explorations\\recommendation-systems-exploration\\venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\github\\explorations\\recommendation-systems-exploration\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:213\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\github\\explorations\\recommendation-systems-exploration\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:85\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     87\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\github\\explorations\\recommendation-systems-exploration\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [100, 2]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "results = {} # {data_context: [{strategy_name, accuracy, precision, recall, f1_score, are_roc}]}\n",
    "\n",
    "for product_data in product_datas:\n",
    "    pprint.pprint(product_data)\n",
    "    print(\"looking at\", \"../\" + product_data[\"product_filepath\"])\n",
    "\n",
    "    productdf =  pd.read_csv(\"../\" + product_data[\"product_filepath\"])\n",
    "    transactiondf = pd.read_csv(\"../\" + product_data[\"transactions_filepath\"])\n",
    "    \n",
    "    training_df_arr = []\n",
    "    \n",
    "    \n",
    "    # join transactions by same user_id. into a dict of user_id: [transactions]\n",
    "    user_transactions = {}\n",
    "    expected_hits = []\n",
    "    hits = []\n",
    "    for row in transactiondf[:100].iterrows():\n",
    "        training_df_arr.append(row[1])\n",
    "        user_id = row[1][\"user_id\"]\n",
    "        if user_id not in user_transactions:\n",
    "            user_transactions[user_id] = []\n",
    "        user_transactions[user_id].append(row[1]['product_id'])\n",
    "        expected_hits.append(1)\n",
    "    \n",
    "    # create df from transactionsdf\n",
    "    transactiondf = pd.DataFrame(training_df_arr)\n",
    "    \n",
    "    # print(\"user_transactions\", len(user_transactions))\n",
    "    # print(\"transactiondf\", transactiondf.shape)\n",
    "    \n",
    "    # print(user_transactions.values())\n",
    "    \n",
    "    # split trainning and testing transactions\n",
    "    past_transactions, test_transactions = train_test_split(list(user_transactions.values()), test_size=.2, random_state=42)\n",
    "    # Display the shapes of the resulting DataFrames\n",
    "    # print(\"Training set shape:\", len(past_transactions))\n",
    "    # print(\"Testing set shape:\", len(test_transactions))\n",
    "    \n",
    "    # for each engine rec. Train, test:\n",
    "    for rec_engine_class in engines_list[:1]:\n",
    "        print(rec_engine_class.strategy_name)\n",
    "        rec_engine: RecommendationAbstract  = rec_engine_class(products=productdf, product_data=product_data, transactions = transactiondf)\n",
    "        rec_engine.train(auto_save=False)\n",
    "        \n",
    "        for user_transactions in test_transactions:\n",
    "            # Start test setup.\n",
    "            # split in 3/4 if more than 4 transactions, or 1/2 if less than 4 transactions\n",
    "            past_transactions, pred_transactions = train_test_split(user_transactions, test_size=.25, random_state=42)\n",
    "            # print('========= ', len(user_transactions), '=========', len(past_transactions), len(pred_transactions))\n",
    "            # print('attempting past_transactions', past_transactions)\n",
    "            recs: List[Tuple[dict, float]] = rec_engine.recommend_from_past(past_transactions)\n",
    "            # get the recommendations as a list of product_ids\n",
    "            recommendation_ids = [rec[0]['product_id'] for rec in recs]\n",
    "            print('recommendation_ids', recommendation_ids)\n",
    "            # If any of the recommendations are in the pred_transactions, then it's a hit\n",
    "            hit = 0\n",
    "            for rec in recommendation_ids:\n",
    "                if rec in pred_transactions:\n",
    "                    hit = 1\n",
    "                    break\n",
    "            hits.append(hit)\n",
    "    \n",
    "    # Measure the performance of the recommendation engine\n",
    "    accuracy = accuracy_score(expected_hits, hits)\n",
    "    precision = precision_score(expected_hits, hits)\n",
    "    recall = recall_score(expected_hits, hits)\n",
    "    f1 = f1_score(expected_hits, hits)\n",
    "    \n",
    "    results[product_data[\"data_context\"]] = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "    \n",
    "print(results)\n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
